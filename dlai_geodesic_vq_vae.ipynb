{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNufLqqBwNC5s2MWaDVjez4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martalombardi/DLAI-Geodesic-VQ-VAE/blob/main/dlai_geodesic_vq_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FIX: binary incompatibility (numpy <-> scikit-learn-extra)\n",
        "# Use pinned versions that work well together.\n",
        "# IMPORTANT: this cell restarts the kernel at the end.\n",
        "# ============================================================\n",
        "\n",
        "!pip -q uninstall -y numpy scikit-learn scikit-learn-extra\n",
        "!pip -q install \"numpy<2\" \"scikit-learn==1.3.2\" \"scikit-learn-extra==0.3.0\" scipy\n",
        "\n",
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "xZwXFQexjQWa",
        "outputId": "c4b507f1-f188-410d-c95a-4a726040b1ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spopt 0.7.0 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "imbalanced-learn 0.14.1 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires scikit-learn>=1.4, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "libpysal 4.14.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "mapclassify 2.10.0 requires scikit-learn>=1.4, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "esda 2.8.1 requires scikit-learn>=1.4, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, sklearn\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "print(\"numpy:\", np.__version__)\n",
        "print(\"sklearn:\", sklearn.__version__)\n",
        "print(\"KMedoids OK ✅\")"
      ],
      "metadata": {
        "id": "N6_ciwQojSlU",
        "outputId": "26d23aa8-2e90-436d-d898-346f79577a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.26.4\n",
            "sklearn: 1.3.2\n",
            "KMedoids OK ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BDce6dM1Noa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71da227c-3a57-4e15-c9ce-87e68380dc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing repository at /content/DLAI-Geodesic-VQ-VAE\n",
            "Cloning repository...\n",
            "Cloning into 'DLAI-Geodesic-VQ-VAE'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "Receiving objects: 100% (17/17), 6.67 KiB | 3.33 MiB/s, done.\n",
            "remote: Total 17 (delta 6), reused 2 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Current working directory: /content/DLAI-Geodesic-VQ-VAE\n",
            "Repository contents:\n",
            "dlai_geodesic_vq_vae.ipynb  LICENSE  README.md\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Bootstrap: clone GitHub repository into Colab runtime\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://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@github.com/martalombardi/DLAI-Geodesic-VQ-VAE.git\"\n",
        "REPO_NAME = \"DLAI-Geodesic-VQ-VAE\"\n",
        "BASE_DIR = \"/content\"\n",
        "\n",
        "repo_path = os.path.join(BASE_DIR, REPO_NAME)\n",
        "\n",
        "# Remove existing clone (if any) to avoid conflicts\n",
        "if os.path.exists(repo_path):\n",
        "    print(f\"Removing existing repository at {repo_path}\")\n",
        "    !rm -rf \"{repo_path}\"\n",
        "\n",
        "# Clone the repository\n",
        "print(\"Cloning repository...\")\n",
        "!git clone \"{REPO_URL}\"\n",
        "\n",
        "# Move into the repository\n",
        "os.chdir(repo_path)\n",
        "\n",
        "# Sanity check\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Repository contents:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Continuous GridVAE (latent grid) — MNIST / FashionMNIST / CIFAR10\n",
        "# Notebook-friendly, reproducible, and formally structured.\n",
        "#\n",
        "# - Encoder/Decoder: conv stride-2 twice -> latent grid (7x7 for 28x28, 8x8 for 32x32)\n",
        "# - Decoder outputs logits (no Sigmoid) for numerical stability (BCEWithLogits).\n",
        "# - Training reports: total / recon / KL (per-sample) for both train and test.\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Reproducibility\n",
        "# ----------------------------\n",
        "def seed_everything(seed: int = 42, deterministic: bool = True) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        try:\n",
        "            torch.use_deterministic_algorithms(True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Experiment configuration\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    seed: int = 42\n",
        "    deterministic: bool = True\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-3\n",
        "    epochs: int = 10\n",
        "    beta: float = 1.0\n",
        "    num_workers: int = 2\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Data loading (MNIST / FashionMNIST / CIFAR10)\n",
        "# ----------------------------\n",
        "def get_dataset_spec(name: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Returns dataset-specific parameters:\n",
        "    - torchvision dataset class\n",
        "    - image size\n",
        "    - channels\n",
        "    - recommended reconstruction loss\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    if name == \"mnist\":\n",
        "        return {\"cls\": datasets.MNIST, \"size\": 28, \"channels\": 1, \"recon_loss\": \"bce\"}\n",
        "    if name in [\"fashionmnist\", \"fashion-mnist\", \"fashion_mnist\"]:\n",
        "        return {\"cls\": datasets.FashionMNIST, \"size\": 28, \"channels\": 1, \"recon_loss\": \"bce\"}\n",
        "    if name == \"cifar10\":\n",
        "        return {\"cls\": datasets.CIFAR10, \"size\": 32, \"channels\": 3, \"recon_loss\": \"mse\"}\n",
        "    raise ValueError(f\"Unsupported dataset: {name}. Use MNIST, FashionMNIST, CIFAR10.\")\n",
        "\n",
        "\n",
        "def make_dataloaders(dataset_name: str,\n",
        "                     data_root: str,\n",
        "                     batch_size: int,\n",
        "                     num_workers: int) -> Tuple[DataLoader, DataLoader, Dict]:\n",
        "    spec = get_dataset_spec(dataset_name)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((spec[\"size\"], spec[\"size\"])),\n",
        "        transforms.ToTensor(),   # keeps data in [0,1]\n",
        "    ])\n",
        "\n",
        "    train_ds = spec[\"cls\"](root=data_root, train=True, download=True, transform=transform)\n",
        "    test_ds  = spec[\"cls\"](root=data_root, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "    return train_loader, test_loader, spec\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Model definition: GridVAE\n",
        "# ----------------------------\n",
        "class GridVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Continuous VAE producing a latent grid (embedding_dim x H' x W').\n",
        "    The decoder returns logits (no sigmoid) for stable BCEWithLogits.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, embedding_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Downsample by factor 4: 28->7, 32->8\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, embedding_dim * 2, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(embedding_dim, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(32, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        h = self.encoder(x)\n",
        "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_logits = self.decoder(z)\n",
        "        return x_logits, mu, logvar\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Loss function\n",
        "# ----------------------------\n",
        "def vae_loss(x_logits: torch.Tensor,\n",
        "             x: torch.Tensor,\n",
        "             mu: torch.Tensor,\n",
        "             logvar: torch.Tensor,\n",
        "             recon_loss: str = \"bce\",\n",
        "             beta: float = 1.0):\n",
        "    \"\"\"\n",
        "    Returns: total, recon, kld (all SUM over batch and pixels), for later normalization.\n",
        "    \"\"\"\n",
        "    if recon_loss == \"bce\":\n",
        "        recon = F.binary_cross_entropy_with_logits(x_logits, x, reduction=\"sum\")\n",
        "    elif recon_loss == \"mse\":\n",
        "        # compare in pixel space (use sigmoid to map logits -> [0,1])\n",
        "        recon = F.mse_loss(torch.sigmoid(x_logits), x, reduction=\"sum\")\n",
        "    else:\n",
        "        raise ValueError(\"recon_loss must be 'bce' or 'mse'.\")\n",
        "\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    total = recon + beta * kld\n",
        "    return total, recon, kld\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Train & evaluate loops\n",
        "# ----------------------------\n",
        "def run_one_epoch(model, loader, optimizer, cfg: TrainConfig, recon_loss: str, train: bool):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_sum = recon_sum = kld_sum = 0.0\n",
        "\n",
        "    # Enable/disable grads\n",
        "    torch.set_grad_enabled(train)\n",
        "    for x, _ in loader:\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        x_logits, mu, logvar = model(x)\n",
        "        loss, recon, kld = vae_loss(x_logits, x, mu, logvar, recon_loss=recon_loss, beta=cfg.beta)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_sum += loss.item()\n",
        "        recon_sum += recon.item()\n",
        "        kld_sum += kld.item()\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    n = len(loader.dataset)\n",
        "    return (total_sum / n), (recon_sum / n), (kld_sum / n)\n",
        "\n",
        "\n",
        "def train_on_dataset(dataset_name: str,\n",
        "                     cfg: TrainConfig,\n",
        "                     data_root: str = \"./data\",\n",
        "                     embedding_dim: int = 64):\n",
        "    \"\"\"\n",
        "    Trains a GridVAE on the specified dataset and prints per-epoch metrics.\n",
        "    \"\"\"\n",
        "    train_loader, test_loader, spec = make_dataloaders(\n",
        "        dataset_name=dataset_name,\n",
        "        data_root=data_root,\n",
        "        batch_size=cfg.batch_size,\n",
        "        num_workers=cfg.num_workers\n",
        "    )\n",
        "\n",
        "    model = GridVAE(in_channels=spec[\"channels\"], embedding_dim=embedding_dim).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"DATASET: {dataset_name} | channels={spec['channels']} | size={spec['size']} | recon_loss={spec['recon_loss']}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        tr_total, tr_recon, tr_kld = run_one_epoch(model, train_loader, optimizer, cfg, spec[\"recon_loss\"], train=True)\n",
        "        te_total, te_recon, te_kld = run_one_epoch(model, test_loader, optimizer, cfg, spec[\"recon_loss\"], train=False)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"Train: total={tr_total:.4f} recon={tr_recon:.4f} kld={tr_kld:.4f} | \"\n",
        "            f\"Test:  total={te_total:.4f} recon={te_recon:.4f} kld={te_kld:.4f}\"\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Run all 3 datasets sequentially\n",
        "# ----------------------------\n",
        "cfg = TrainConfig(\n",
        "    seed=42,\n",
        "    deterministic=True,\n",
        "    batch_size=128,\n",
        "    lr=1e-3,\n",
        "    epochs=10,     # aumenta pure a 20 quando vuoi\n",
        "    beta=1.0,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "seed_everything(cfg.seed, cfg.deterministic)\n",
        "\n",
        "models = {}\n",
        "for ds in [\"MNIST\", \"FashionMNIST\", \"CIFAR10\"]:\n",
        "    models[ds] = train_on_dataset(ds, cfg, data_root=\"./data\", embedding_dim=64)\n"
      ],
      "metadata": {
        "id": "spcYgPnVdxRL",
        "outputId": "81dc1f43-104a-4983-bfdf-5d5d8c4290ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 486kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 15.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET: MNIST | channels=1 | size=28 | recon_loss=bce\n",
            "======================================================================\n",
            "Epoch 01 | Train: total=192.7869 recon=148.1763 kld=44.6105 | Test:  total=148.8501 recon=98.3906 kld=50.4594\n",
            "Epoch 02 | Train: total=136.7548 recon=87.9305 kld=48.8242 | Test:  total=131.6048 recon=83.8580 kld=47.7468\n",
            "Epoch 03 | Train: total=130.7587 recon=83.3205 kld=47.4381 | Test:  total=129.0360 recon=81.3152 kld=47.7208\n",
            "Epoch 04 | Train: total=128.8068 recon=81.8064 kld=47.0004 | Test:  total=127.3928 recon=80.8713 kld=46.5215\n",
            "Epoch 05 | Train: total=127.8114 recon=80.9866 kld=46.8248 | Test:  total=126.7313 recon=79.7407 kld=46.9907\n",
            "Epoch 06 | Train: total=126.8487 recon=80.1963 kld=46.6524 | Test:  total=126.5308 recon=80.7324 kld=45.7983\n",
            "Epoch 07 | Train: total=126.2020 recon=79.7014 kld=46.5006 | Test:  total=125.4645 recon=79.0132 kld=46.4513\n",
            "Epoch 08 | Train: total=125.7408 recon=79.3391 kld=46.4017 | Test:  total=125.1393 recon=79.5349 kld=45.6044\n",
            "Epoch 09 | Train: total=125.2177 recon=78.9595 kld=46.2583 | Test:  total=124.9174 recon=81.3391 kld=43.5783\n",
            "Epoch 10 | Train: total=124.9134 recon=78.8036 kld=46.1097 | Test:  total=124.2927 recon=78.4195 kld=45.8732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 212kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.95MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET: FashionMNIST | channels=1 | size=28 | recon_loss=bce\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train: total=327.1934 recon=292.1047 kld=35.0887 | Test:  total=312.4681 recon=275.5447 kld=36.9235\n",
            "Epoch 02 | Train: total=296.6275 recon=257.3473 kld=39.2802 | Test:  total=288.1103 recon=248.3747 kld=39.7357\n",
            "Epoch 03 | Train: total=284.4896 recon=243.5270 kld=40.9626 | Test:  total=284.6010 recon=244.0521 kld=40.5489\n",
            "Epoch 04 | Train: total=282.3816 recon=241.1586 kld=41.2231 | Test:  total=283.2340 recon=242.9437 kld=40.2902\n",
            "Epoch 05 | Train: total=281.1320 recon=239.9253 kld=41.2068 | Test:  total=282.3301 recon=240.6937 kld=41.6364\n",
            "Epoch 06 | Train: total=280.3599 recon=239.2094 kld=41.1506 | Test:  total=281.4668 recon=241.1906 kld=40.2762\n",
            "Epoch 07 | Train: total=279.6878 recon=238.5739 kld=41.1139 | Test:  total=281.3178 recon=240.8649 kld=40.4529\n",
            "Epoch 08 | Train: total=279.3150 recon=238.1488 kld=41.1662 | Test:  total=280.6050 recon=238.8990 kld=41.7059\n",
            "Epoch 09 | Train: total=278.9146 recon=237.7446 kld=41.1700 | Test:  total=280.1985 recon=240.0527 kld=40.1459\n",
            "Epoch 10 | Train: total=278.5213 recon=237.3469 kld=41.1744 | Test:  total=280.0808 recon=239.5402 kld=40.5406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATASET: CIFAR10 | channels=3 | size=32 | recon_loss=mse\n",
            "======================================================================\n",
            "Epoch 01 | Train: total=132.1218 recon=111.9094 kld=20.2125 | Test:  total=114.6179 recon=89.4871 kld=25.1308\n",
            "Epoch 02 | Train: total=113.1527 recon=87.1566 kld=25.9961 | Test:  total=112.4026 recon=85.6808 kld=26.7218\n",
            "Epoch 03 | Train: total=110.3136 recon=82.8661 kld=27.4475 | Test:  total=104.1332 recon=73.1979 kld=30.9353\n",
            "Epoch 04 | Train: total=101.8929 recon=69.4407 kld=32.4521 | Test:  total=100.8598 recon=67.1577 kld=33.7022\n",
            "Epoch 05 | Train: total=100.7199 recon=66.9194 kld=33.8004 | Test:  total=100.1586 recon=66.5370 kld=33.6216\n",
            "Epoch 06 | Train: total=100.2269 recon=66.0657 kld=34.1613 | Test:  total=100.1025 recon=66.5957 kld=33.5068\n",
            "Epoch 07 | Train: total=99.9377 recon=65.6250 kld=34.3128 | Test:  total=99.5198 recon=64.1603 kld=35.3595\n",
            "Epoch 08 | Train: total=99.7378 recon=65.2904 kld=34.4474 | Test:  total=99.4400 recon=64.2330 kld=35.2070\n",
            "Epoch 09 | Train: total=99.4977 recon=64.9323 kld=34.5654 | Test:  total=100.1104 recon=65.1705 kld=34.9399\n",
            "Epoch 10 | Train: total=99.4107 recon=64.7857 kld=34.6250 | Test:  total=99.1895 recon=64.2965 kld=34.8929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# A Posteriori Geodesic Quantization (Method A) — with KMedoids\n",
        "# - multi-dataset: MNIST / FashionMNIST / CIFAR10\n",
        "# - streaming extraction (no 3M vectors stored)\n",
        "# - auto latent grid size (7x7 for 28x28, 8x8 for 32x32)\n",
        "# ============================================================\n",
        "\n",
        "import os, json, time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from scipy.sparse.csgraph import shortest_path, connected_components\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Latents (mu) in streaming\n",
        "# ----------------------------\n",
        "def _latent_batch_mu(model, x, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = x.to(device)\n",
        "        _, mu, _ = model(x)   # mu: (B, C, H_lat, W_lat)\n",
        "    return mu\n",
        "\n",
        "def reservoir_sample_landmarks(model, loader, device, sample_size=5000, seed=42):\n",
        "    \"\"\"\n",
        "    Reservoir sampling of latent vectors without storing all vectors.\n",
        "    Returns landmarks (S,C) and latent grid size (H_lat,W_lat).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    landmarks = None\n",
        "    seen = 0\n",
        "    H_lat = W_lat = None\n",
        "    C = None\n",
        "    n_images = 0\n",
        "\n",
        "    for x, _ in loader:\n",
        "        mu = _latent_batch_mu(model, x, device)\n",
        "        B, Cb, Hb, Wb = mu.shape\n",
        "        if landmarks is None:\n",
        "            C, H_lat, W_lat = Cb, Hb, Wb\n",
        "            landmarks = np.empty((sample_size, C), dtype=np.float32)\n",
        "\n",
        "        vecs = mu.permute(0, 2, 3, 1).reshape(-1, C).cpu().numpy().astype(np.float32)\n",
        "\n",
        "        for v in vecs:\n",
        "            if seen < sample_size:\n",
        "                landmarks[seen] = v\n",
        "            else:\n",
        "                j = rng.integers(0, seen + 1)\n",
        "                if j < sample_size:\n",
        "                    landmarks[j] = v\n",
        "            seen += 1\n",
        "\n",
        "        n_images += B\n",
        "\n",
        "    return landmarks, (H_lat, W_lat), C, n_images, seen\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Build connected kNN graph\n",
        "# ----------------------------\n",
        "def build_connected_knn_graph(X, n_neighbors=15):\n",
        "    G = kneighbors_graph(X, n_neighbors=n_neighbors, mode=\"distance\", include_self=False, n_jobs=-1)\n",
        "    G = 0.5 * (G + G.T)  # symmetrize\n",
        "\n",
        "    n_comp, labels = connected_components(G, directed=False)\n",
        "    if n_comp <= 1:\n",
        "        return G\n",
        "\n",
        "    print(f\"[WARN] kNN graph disconnected: {n_comp} components. Distances may include inf -> will be fixed.\")\n",
        "    return G\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Geodesic distances among landmarks\n",
        "# ----------------------------\n",
        "def geodesic_distances(landmarks, n_neighbors=15):\n",
        "    G = build_connected_knn_graph(landmarks, n_neighbors=n_neighbors)\n",
        "    D = shortest_path(G, directed=False, method=\"D\").astype(np.float32)\n",
        "\n",
        "    finite = np.isfinite(D)\n",
        "    if not np.all(finite):\n",
        "        max_fin = np.max(D[finite])\n",
        "        D[~finite] = max_fin * 1.1\n",
        "        print(\"[WARN] inf distances detected -> replaced with max_finite*1.1\")\n",
        "    return D\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Quantize full dataset streaming\n",
        "# ----------------------------\n",
        "def quantize_streaming(model, loader, device, landmarks, landmark_labels):\n",
        "    nn = NearestNeighbors(n_neighbors=1).fit(landmarks)\n",
        "\n",
        "    codes_all = []\n",
        "    H_lat = W_lat = None\n",
        "\n",
        "    for x, _ in loader:\n",
        "        mu = _latent_batch_mu(model, x, device)  # (B,C,H,W)\n",
        "        B, C, H, W = mu.shape\n",
        "        if H_lat is None:\n",
        "            H_lat, W_lat = H, W\n",
        "\n",
        "        vecs = mu.permute(0, 2, 3, 1).reshape(-1, C).cpu().numpy().astype(np.float32)\n",
        "        _, idx = nn.kneighbors(vecs, return_distance=True)\n",
        "        idx = idx[:, 0]\n",
        "        codes = landmark_labels[idx].reshape(B, H, W).astype(np.int32)\n",
        "        codes_all.append(codes)\n",
        "\n",
        "    return np.concatenate(codes_all, axis=0)  # (N,H,W)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# End-to-end for one dataset\n",
        "# ----------------------------\n",
        "def geodesic_quantize_method_A(\n",
        "    dataset_name: str,\n",
        "    model,\n",
        "    full_train_loader,\n",
        "    out_dir=\"results/quantizer\",\n",
        "    seed=42,\n",
        "    sample_size=5000,\n",
        "    n_neighbors=15,\n",
        "    n_codes=128,\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    run_tag = f\"{dataset_name}_S{sample_size}_k{n_neighbors}_K{n_codes}_seed{seed}_{int(time.time())}\"\n",
        "    save_path = os.path.join(out_dir, run_tag)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"GEODESIC QUANTIZATION — {dataset_name}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"[A] Sampling landmarks (reservoir)...\")\n",
        "    landmarks, (H_lat, W_lat), C, n_images, total_vecs = reservoir_sample_landmarks(\n",
        "        model, full_train_loader, DEVICE, sample_size=sample_size, seed=seed\n",
        "    )\n",
        "    print(f\"Landmarks: {landmarks.shape} | latent grid: {H_lat}x{W_lat} | images: {n_images} | total vectors: {total_vecs}\")\n",
        "\n",
        "    print(\"[B] Geodesic distances among landmarks...\")\n",
        "    D_geo = geodesic_distances(landmarks, n_neighbors=n_neighbors)\n",
        "\n",
        "    print(\"[C] KMedoids on precomputed geodesic distances...\")\n",
        "    kmed = KMedoids(n_clusters=n_codes, metric=\"precomputed\", random_state=seed).fit(D_geo)\n",
        "    codebook = landmarks[kmed.medoid_indices_].astype(np.float32)\n",
        "    landmark_labels = kmed.labels_.astype(np.int32)\n",
        "    print(\"✅ Codebook:\", codebook.shape)\n",
        "\n",
        "    print(\"[D] Quantize full train set (streaming)...\")\n",
        "    codes = quantize_streaming(model, full_train_loader, DEVICE, landmarks, landmark_labels)\n",
        "    print(\"✅ Codes shape:\", codes.shape, \"(N, H_lat, W_lat)\")\n",
        "\n",
        "    meta = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"seed\": seed,\n",
        "        \"sample_size\": sample_size,\n",
        "        \"n_neighbors\": n_neighbors,\n",
        "        \"n_codes\": n_codes,\n",
        "        \"latent_grid\": [int(H_lat), int(W_lat)],\n",
        "        \"embedding_dim\": int(C),\n",
        "        \"n_images\": int(codes.shape[0]),\n",
        "    }\n",
        "    with open(os.path.join(save_path, \"meta.json\"), \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "\n",
        "    np.save(os.path.join(save_path, \"landmarks.npy\"), landmarks)\n",
        "    np.save(os.path.join(save_path, \"landmark_labels.npy\"), landmark_labels)\n",
        "    np.save(os.path.join(save_path, \"codebook.npy\"), codebook)\n",
        "    np.save(os.path.join(save_path, \"codes_train.npy\"), codes)\n",
        "\n",
        "    print(f\"\\nSaved to: {save_path}\\n\")\n",
        "    return save_path\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Full train loader per dataset (shuffle=False)\n",
        "# ----------------------------\n",
        "def make_full_train_loader(dataset_name, batch_size=128, data_root=\"./data\"):\n",
        "    if dataset_name in [\"MNIST\", \"FashionMNIST\"]:\n",
        "        size = 28\n",
        "    else:\n",
        "        size = 32\n",
        "\n",
        "    tfm = transforms.Compose([transforms.Resize((size, size)), transforms.ToTensor()])\n",
        "\n",
        "    if dataset_name == \"MNIST\":\n",
        "        ds = datasets.MNIST(data_root, train=True, download=True, transform=tfm)\n",
        "    elif dataset_name == \"FashionMNIST\":\n",
        "        ds = datasets.FashionMNIST(data_root, train=True, download=True, transform=tfm)\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        ds = datasets.CIFAR10(data_root, train=True, download=True, transform=tfm)\n",
        "    else:\n",
        "        raise ValueError(dataset_name)\n",
        "\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Run for all 3 datasets\n",
        "# Assumes you already have: models[\"MNIST\"], models[\"FashionMNIST\"], models[\"CIFAR10\"]\n",
        "# ----------------------------\n",
        "quant_cfg = {\n",
        "    \"MNIST\":        dict(sample_size=5000, n_neighbors=15, n_codes=128),\n",
        "    \"FashionMNIST\": dict(sample_size=5000, n_neighbors=20, n_codes=256),\n",
        "    \"CIFAR10\":      dict(sample_size=6000, n_neighbors=25, n_codes=512),\n",
        "}\n",
        "\n",
        "quantizer_paths = {}\n",
        "for ds in [\"MNIST\", \"FashionMNIST\", \"CIFAR10\"]:\n",
        "    full_loader = make_full_train_loader(ds, batch_size=128, data_root=\"./data\")\n",
        "    quantizer_paths[ds] = geodesic_quantize_method_A(\n",
        "        dataset_name=ds,\n",
        "        model=models[ds],\n",
        "        full_train_loader=full_loader,\n",
        "        out_dir=\"results/quantizer\",\n",
        "        seed=42,\n",
        "        **quant_cfg[ds]\n",
        "    )\n",
        "\n",
        "print(\"Done. Quantizer paths:\")\n",
        "for k, v in quantizer_paths.items():\n",
        "    print(k, \"->\", v)\n"
      ],
      "metadata": {
        "id": "CUpMIN56hF_Z",
        "outputId": "8a01bc65-768b-420c-d161-4b6de10a801a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GEODESIC QUANTIZATION — MNIST\n",
            "======================================================================\n",
            "[A] Sampling landmarks (reservoir)...\n",
            "Landmarks: (5000, 64) | latent grid: 7x7 | images: 60000 | total vectors: 2940000\n",
            "[B] Geodesic distances among landmarks...\n",
            "[C] KMedoids on precomputed geodesic distances...\n",
            "✅ Codebook: (128, 64)\n",
            "[D] Quantize full train set (streaming)...\n",
            "✅ Codes shape: (60000, 7, 7) (N, H_lat, W_lat)\n",
            "\n",
            "Saved to: results/quantizer/MNIST_S5000_k15_K128_seed42_1769687155\n",
            "\n",
            "\n",
            "======================================================================\n",
            "GEODESIC QUANTIZATION — FashionMNIST\n",
            "======================================================================\n",
            "[A] Sampling landmarks (reservoir)...\n",
            "Landmarks: (5000, 64) | latent grid: 7x7 | images: 60000 | total vectors: 2940000\n",
            "[B] Geodesic distances among landmarks...\n",
            "[C] KMedoids on precomputed geodesic distances...\n",
            "✅ Codebook: (256, 64)\n",
            "[D] Quantize full train set (streaming)...\n",
            "✅ Codes shape: (60000, 7, 7) (N, H_lat, W_lat)\n",
            "\n",
            "Saved to: results/quantizer/FashionMNIST_S5000_k20_K256_seed42_1769687301\n",
            "\n",
            "\n",
            "======================================================================\n",
            "GEODESIC QUANTIZATION — CIFAR10\n",
            "======================================================================\n",
            "[A] Sampling landmarks (reservoir)...\n",
            "Landmarks: (6000, 64) | latent grid: 8x8 | images: 50000 | total vectors: 3200000\n",
            "[B] Geodesic distances among landmarks...\n",
            "[C] KMedoids on precomputed geodesic distances...\n",
            "✅ Codebook: (512, 64)\n",
            "[D] Quantize full train set (streaming)...\n",
            "✅ Codes shape: (50000, 8, 8) (N, H_lat, W_lat)\n",
            "\n",
            "Saved to: results/quantizer/CIFAR10_S6000_k25_K512_seed42_1769687455\n",
            "\n",
            "Done. Quantizer paths:\n",
            "MNIST -> results/quantizer/MNIST_S5000_k15_K128_seed42_1769687155\n",
            "FashionMNIST -> results/quantizer/FashionMNIST_S5000_k20_K256_seed42_1769687301\n",
            "CIFAR10 -> results/quantizer/CIFAR10_S6000_k25_K512_seed42_1769687455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# =========================\n",
        "# 0) Config minimale\n",
        "# =========================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "QUANT_DIR = \"results/quantizer/MNIST_S5000_k15_K128_seed42_1769687155\"   # <-- cambia qui\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "LR = 3e-4\n",
        "D_MODEL = 256\n",
        "N_HEADS = 8\n",
        "N_LAYERS = 4\n",
        "DROPOUT = 0.1\n",
        "SEED = 42\n",
        "VAL_FRAC = 0.05\n",
        "\n",
        "# =========================\n",
        "# 0b) Riproducibilità \"pratica\" (NO deterministic-algorithms su CUDA)\n",
        "#     Evita l'errore CuBLAS con MultiHeadAttention.\n",
        "# =========================\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Non forzare determinismo totale su CUDA: causa RuntimeError con CuBLAS\n",
        "    torch.use_deterministic_algorithms(False)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "seed_everything(SEED)\n",
        "\n",
        "# =========================\n",
        "# 1) Carica codici + meta\n",
        "# =========================\n",
        "meta_path = os.path.join(QUANT_DIR, \"meta.json\")\n",
        "codes_path = os.path.join(QUANT_DIR, \"codes_train.npy\")\n",
        "\n",
        "assert os.path.exists(meta_path), f\"meta.json non trovato: {meta_path}\"\n",
        "assert os.path.exists(codes_path), f\"codes_train.npy non trovato: {codes_path}\"\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "codes = np.load(codes_path)  # (N, H, W)\n",
        "K = int(meta[\"n_codes\"])\n",
        "H, W = meta[\"latent_grid\"]\n",
        "T = H * W\n",
        "\n",
        "BOS = K              # token di inizio sequenza\n",
        "VOCAB = K + 1        # include BOS\n",
        "\n",
        "print(\"codes:\", codes.shape, \"| K:\", K, \"| HxW:\", (H, W), \"| T:\", T, \"| device:\", DEVICE)\n",
        "\n",
        "# =========================\n",
        "# 2) Dataset (shifted)\n",
        "# =========================\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, codes_3d: np.ndarray, K: int):\n",
        "        self.codes = codes_3d.astype(np.int64)\n",
        "        self.K = K\n",
        "        self.bos = K\n",
        "        self.T = self.codes.shape[1] * self.codes.shape[2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.codes.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y = self.codes[idx].reshape(-1)  # (T,)\n",
        "        x = np.empty_like(y)\n",
        "        x[0] = self.bos\n",
        "        x[1:] = y[:-1]\n",
        "        return torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
        "\n",
        "# split semplice train/val (riproducibile)\n",
        "N = len(codes)\n",
        "perm = np.random.permutation(N)\n",
        "val_n = max(1, int(VAL_FRAC * N))\n",
        "val_idx = perm[:val_n]\n",
        "tr_idx  = perm[val_n:]\n",
        "\n",
        "train_ds = CodeDataset(codes[tr_idx], K)\n",
        "val_ds   = CodeDataset(codes[val_idx], K)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# =========================\n",
        "# 3) Transformer semplice (decoder-only via causal mask)\n",
        "# =========================\n",
        "class SimpleARTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, K: int, T: int,\n",
        "                 d_model=256, n_heads=8, n_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.T = T\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(T, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, K)  # prediciamo solo 0..K-1 (no BOS)\n",
        "\n",
        "        # causal mask (T,T): blocca il futuro\n",
        "        mask = torch.full((T, T), float(\"-inf\"))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        B, Tcur = x.shape\n",
        "        pos = torch.arange(Tcur, device=x.device).unsqueeze(0)  # (1,T)\n",
        "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
        "        h = self.tr(h, mask=self.causal_mask[:Tcur, :Tcur])\n",
        "        h = self.ln(h)\n",
        "        logits = self.head(h)  # (B,T,K)\n",
        "        return logits\n",
        "\n",
        "model = SimpleARTransformer(\n",
        "    vocab_size=VOCAB, K=K, T=T,\n",
        "    d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, dropout=DROPOUT\n",
        ").to(DEVICE)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# =========================\n",
        "# 4) Train loop (semplice)\n",
        "# =========================\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "            if train:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            logits = model(x)  # (B,T,K)\n",
        "            loss = loss_fn(logits.reshape(-1, K), y.reshape(-1))\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    avg_loss_per_sample = total_loss / len(loader.dataset)\n",
        "    nll_per_token = avg_loss_per_sample / T\n",
        "    ppl = math.exp(min(20.0, nll_per_token))\n",
        "    return avg_loss_per_sample, ppl\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr_loss, tr_ppl = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_ppl = run_epoch(val_loader, train=False)\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} ppl={tr_ppl:.3f} | val_loss={va_loss:.4f} ppl={va_ppl:.3f}\")\n",
        "\n",
        "# =========================\n",
        "# 5) Save\n",
        "# =========================\n",
        "save_path = os.path.join(QUANT_DIR, \"transformer_prior.pt\")\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Saved:\", save_path)\n"
      ],
      "metadata": {
        "id": "d5HkRHhPmvNC",
        "outputId": "6ce07912-88a1-441b-a002-d1d30f939347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "codes: (60000, 7, 7) | K: 128 | HxW: (7, 7) | T: 49 | device: cuda\n",
            "Epoch 01 | train_loss=2.4395 ppl=1.051 | val_loss=2.0187 ppl=1.042\n",
            "Epoch 02 | train_loss=1.9278 ppl=1.040 | val_loss=1.8046 ppl=1.038\n",
            "Epoch 03 | train_loss=1.7972 ppl=1.037 | val_loss=1.7222 ppl=1.036\n",
            "Epoch 04 | train_loss=1.7297 ppl=1.036 | val_loss=1.6749 ppl=1.035\n",
            "Epoch 05 | train_loss=1.6860 ppl=1.035 | val_loss=1.6436 ppl=1.034\n",
            "Epoch 06 | train_loss=1.6530 ppl=1.034 | val_loss=1.6182 ppl=1.034\n",
            "Epoch 07 | train_loss=1.6279 ppl=1.034 | val_loss=1.6003 ppl=1.033\n",
            "Epoch 08 | train_loss=1.6070 ppl=1.033 | val_loss=1.5844 ppl=1.033\n",
            "Epoch 09 | train_loss=1.5892 ppl=1.033 | val_loss=1.5739 ppl=1.033\n",
            "Epoch 10 | train_loss=1.5738 ppl=1.033 | val_loss=1.5645 ppl=1.032\n",
            "Epoch 11 | train_loss=1.5601 ppl=1.032 | val_loss=1.5517 ppl=1.032\n",
            "Epoch 12 | train_loss=1.5478 ppl=1.032 | val_loss=1.5487 ppl=1.032\n",
            "Epoch 13 | train_loss=1.5364 ppl=1.032 | val_loss=1.5396 ppl=1.032\n",
            "Epoch 14 | train_loss=1.5267 ppl=1.032 | val_loss=1.5375 ppl=1.032\n",
            "Epoch 15 | train_loss=1.5177 ppl=1.031 | val_loss=1.5342 ppl=1.032\n",
            "Epoch 16 | train_loss=1.5087 ppl=1.031 | val_loss=1.5263 ppl=1.032\n",
            "Epoch 17 | train_loss=1.5004 ppl=1.031 | val_loss=1.5245 ppl=1.032\n",
            "Epoch 18 | train_loss=1.4929 ppl=1.031 | val_loss=1.5205 ppl=1.032\n",
            "Epoch 19 | train_loss=1.4857 ppl=1.031 | val_loss=1.5202 ppl=1.032\n",
            "Epoch 20 | train_loss=1.4793 ppl=1.031 | val_loss=1.5172 ppl=1.031\n",
            "Saved: results/quantizer/MNIST_S5000_k15_K128_seed42_1769687155/transformer_prior.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# 6) Final Generation Pipeline (Method A)\n",
        "# ============================================================\n",
        "print(f\"Generating images for dataset: {meta['dataset']}...\")\n",
        "\n",
        "# Imposta i modelli in modalità evaluation\n",
        "model.eval()  # Il tuo Transformer\n",
        "vae_model = models[meta['dataset']]\n",
        "vae_model.eval()\n",
        "\n",
        "# Carica il codebook salvato e spostalo sul DEVICE\n",
        "codebook_path = os.path.join(QUANT_DIR, \"codebook.npy\")\n",
        "codebook_np = np.load(codebook_path)\n",
        "codebook_tensor = torch.from_numpy(codebook_np).to(DEVICE)\n",
        "\n",
        "# Parametri dalla meta-configurazione\n",
        "K_codes = int(meta[\"n_codes\"])\n",
        "H_lat, W_lat = meta[\"latent_grid\"]\n",
        "T_seq = H_lat * W_lat\n",
        "BOS_token = K_codes  # Come definito nel tuo training (BOS = K)\n",
        "\n",
        "num_samples = 16\n",
        "\n",
        "# 1. Generazione sequenze autoregressive con il Transformer\n",
        "# Iniziamo con il token BOS\n",
        "generated_seqs = torch.full((num_samples, 1), BOS_token, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "print(\"Sampling tokens...\")\n",
        "for _ in range(T_seq):\n",
        "    with torch.no_grad():\n",
        "        # Il tuo modello accetta x: (B, Tcur) e applica internamente la maschera causale\n",
        "        logits = model(generated_seqs)\n",
        "\n",
        "        # Prendiamo i logits dell'ultimo step: (B, K)\n",
        "        next_token_logits = logits[:, -1, :]\n",
        "\n",
        "        # Sampling (puoi aggiungere temperatura qui se vuoi più varietà)\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append alla sequenza\n",
        "        generated_seqs = torch.cat([generated_seqs, next_token], dim=1)\n",
        "\n",
        "# Rimuoviamo il token BOS iniziale e facciamo il reshape in griglia (H, W)\n",
        "# generated_seqs era (16, T+1), diventa (16, H, W)\n",
        "final_codes = generated_seqs[:, 1:].reshape(num_samples, H_lat, W_lat)\n",
        "\n",
        "# 2. Decoding tramite il Decoder della GridVAE\n",
        "with torch.no_grad():\n",
        "    # Trasformiamo gli indici in vettori continui usando il codebook come look-up table\n",
        "    # final_codes: (B, H, W) -> latents: (B, H, W, C)\n",
        "    latents = F.embedding(final_codes, codebook_tensor)\n",
        "\n",
        "    # GridVAE si aspetta (B, C, H, W)\n",
        "    latents = latents.permute(0, 3, 1, 2)\n",
        "\n",
        "    # Passaggio nel decoder (restituisce logits)\n",
        "    gen_logits = vae_model.decoder(latents)\n",
        "\n",
        "    # Applichiamo Sigmoid per visualizzare in [0, 1]\n",
        "    gen_images = torch.sigmoid(gen_logits).cpu()\n",
        "\n",
        "# 3. Visualizzazione\n",
        "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))\n",
        "plt.suptitle(f\"Generated Samples - {meta['dataset']} (Method A)\")\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = gen_images[i]\n",
        "    if img.shape[0] == 1:  # MNIST / FashionMNIST\n",
        "        ax.imshow(img.squeeze(), cmap='gray')\n",
        "    else:  # CIFAR10\n",
        "        ax.imshow(img.permute(1, 2, 0))\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXkrHyDup5-s",
        "outputId": "68197015-6884-4150-ab4b-74d13aa6268b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating images for dataset: MNIST...\n",
            "Sampling tokens...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, math, random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# =========================\n",
        "# 0) Config minimale\n",
        "# =========================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "QUANT_DIR = \"results/quantizer/FashionMNIST_S5000_k20_K256_seed42_1769687301\"   # <-- cambia qui\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "LR = 3e-4\n",
        "D_MODEL = 256\n",
        "N_HEADS = 8\n",
        "N_LAYERS = 4\n",
        "DROPOUT = 0.1\n",
        "SEED = 42\n",
        "VAL_FRAC = 0.05\n",
        "\n",
        "# =========================\n",
        "# 0b) Riproducibilità \"pratica\" (NO deterministic-algorithms su CUDA)\n",
        "#     Evita l'errore CuBLAS con MultiHeadAttention.\n",
        "# =========================\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Non forzare determinismo totale su CUDA: causa RuntimeError con CuBLAS\n",
        "    torch.use_deterministic_algorithms(False)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "seed_everything(SEED)\n",
        "\n",
        "# =========================\n",
        "# 1) Carica codici + meta\n",
        "# =========================\n",
        "meta_path = os.path.join(QUANT_DIR, \"meta.json\")\n",
        "codes_path = os.path.join(QUANT_DIR, \"codes_train.npy\")\n",
        "\n",
        "assert os.path.exists(meta_path), f\"meta.json non trovato: {meta_path}\"\n",
        "assert os.path.exists(codes_path), f\"codes_train.npy non trovato: {codes_path}\"\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "codes = np.load(codes_path)  # (N, H, W)\n",
        "K = int(meta[\"n_codes\"])\n",
        "H, W = meta[\"latent_grid\"]\n",
        "T = H * W\n",
        "\n",
        "BOS = K              # token di inizio sequenza\n",
        "VOCAB = K + 1        # include BOS\n",
        "\n",
        "print(\"codes:\", codes.shape, \"| K:\", K, \"| HxW:\", (H, W), \"| T:\", T, \"| device:\", DEVICE)\n",
        "\n",
        "# =========================\n",
        "# 2) Dataset (shifted)\n",
        "# =========================\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, codes_3d: np.ndarray, K: int):\n",
        "        self.codes = codes_3d.astype(np.int64)\n",
        "        self.K = K\n",
        "        self.bos = K\n",
        "        self.T = self.codes.shape[1] * self.codes.shape[2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.codes.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y = self.codes[idx].reshape(-1)  # (T,)\n",
        "        x = np.empty_like(y)\n",
        "        x[0] = self.bos\n",
        "        x[1:] = y[:-1]\n",
        "        return torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
        "\n",
        "# split semplice train/val (riproducibile)\n",
        "N = len(codes)\n",
        "perm = np.random.permutation(N)\n",
        "val_n = max(1, int(VAL_FRAC * N))\n",
        "val_idx = perm[:val_n]\n",
        "tr_idx  = perm[val_n:]\n",
        "\n",
        "train_ds = CodeDataset(codes[tr_idx], K)\n",
        "val_ds   = CodeDataset(codes[val_idx], K)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# =========================\n",
        "# 3) Transformer semplice (decoder-only via causal mask)\n",
        "# =========================\n",
        "class SimpleARTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, K: int, T: int,\n",
        "                 d_model=256, n_heads=8, n_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.T = T\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(T, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, K)  # prediciamo solo 0..K-1 (no BOS)\n",
        "\n",
        "        # causal mask (T,T): blocca il futuro\n",
        "        mask = torch.full((T, T), float(\"-inf\"))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        self.register_buffer(\"causal_mask\", mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        B, Tcur = x.shape\n",
        "        pos = torch.arange(Tcur, device=x.device).unsqueeze(0)  # (1,T)\n",
        "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
        "        h = self.tr(h, mask=self.causal_mask[:Tcur, :Tcur])\n",
        "        h = self.ln(h)\n",
        "        logits = self.head(h)  # (B,T,K)\n",
        "        return logits\n",
        "\n",
        "model = SimpleARTransformer(\n",
        "    vocab_size=VOCAB, K=K, T=T,\n",
        "    d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS, dropout=DROPOUT\n",
        ").to(DEVICE)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# =========================\n",
        "# 4) Train loop (semplice)\n",
        "# =========================\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.set_grad_enabled(train):\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "            if train:\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            logits = model(x)  # (B,T,K)\n",
        "            loss = loss_fn(logits.reshape(-1, K), y.reshape(-1))\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    avg_loss_per_sample = total_loss / len(loader.dataset)\n",
        "    nll_per_token = avg_loss_per_sample / T\n",
        "    ppl = math.exp(min(20.0, nll_per_token))\n",
        "    return avg_loss_per_sample, ppl\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr_loss, tr_ppl = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_ppl = run_epoch(val_loader, train=False)\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} ppl={tr_ppl:.3f} | val_loss={va_loss:.4f} ppl={va_ppl:.3f}\")\n",
        "\n",
        "# =========================\n",
        "# 5) Save\n",
        "# =========================\n",
        "save_path = os.path.join(QUANT_DIR, \"transformer_prior.pt\")\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Saved:\", save_path)"
      ],
      "metadata": {
        "id": "ru4QBZays7G2",
        "outputId": "3a05a49a-1841-40d8-c3b4-91ca2e9d3d3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "codes: (60000, 7, 7) | K: 256 | HxW: (7, 7) | T: 49 | device: cuda\n",
            "Epoch 01 | train_loss=2.4297 ppl=1.051 | val_loss=1.9147 ppl=1.040\n",
            "Epoch 02 | train_loss=1.8543 ppl=1.039 | val_loss=1.7349 ppl=1.036\n",
            "Epoch 03 | train_loss=1.7377 ppl=1.036 | val_loss=1.6629 ppl=1.035\n"
          ]
        }
      ]
    }
  ]
}